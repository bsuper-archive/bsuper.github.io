<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>blog.bsu.me/</title>
   
   <link>http://blog.bsu.me/</link>
   <description>A place where I'll share my latest travel stories, academic thoughts, and personal musings.</description>
   <language>en-us</language>
   <managingEditor> Brian Su</managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>Prioritized Experience Replay Kills on Doom</title>
	  <link>//294-per-doom</link>
	  <author>Brian Su</author>
	  <pubDate>2016-12-15T00:00:00+00:00</pubDate>
	  <guid>//294-per-doom</guid>
	  <description><![CDATA[
	     <p>
  <i>By Brian Su, Cem Koc, and Can Koc (Echochrome)</i>
</p>

<h3>Why play video games?</h3>
<p>
  You may wonder why companies like Google and Facebook have research teams that try to outperform others (and themselves) on video games, such as Atari or Doom. I mean, why would some of the best minds of our day spend hours tackling how to be better at video games?
</p>
<p>
  Turns out, the eventual goal is to use reinforcement learning to allow robots and other machines to interact with complex environments. In short, games are a simplified abstraction of a complex environment machines have to learn and adapt to. Once we have successfully trained a model to play a game, applying the model to a real-world problem is a natural extension. For example, Google’s DeepMind has trained reinforcement learning models to master Atari games, beat a 9-dan Go professional, and, more recently, reduced the power consumption of Google’s data centers by 15%. Moreover,  games provide two advantages when used for deep reinforcement learning.
</p>
<p>
  First, since machines can quickly run through games, we are able to accumulate much more data from playing games compared to attempting to collect data in the real-world. In particular, this is beneficial when using deep neural networks they require lots of data to be able to perform successfully.
</p>
<p>
  Second, games provide a visual representation of an environment. When humans play games, they learn how to play and make decisions based on what they see on the screen. Cameras are machines’ equivalent of human eyes, capturing a representation of the environment as an image, and machine learning has been successfully done to detect objects in the images. Thus, for humans and these machine learning models, the input is the raw pixels from images. For the task of deep reinforcement learning, most games provide a visual representation capturing most of the state of the game. Like how humans learn to play games, the goal is to use deep reinforcement learning and allow machines only use the visual representation to successfully complete the game. This is often referred to as end-to-end training.
</p>
<p>
  CAD2RL is one recent example where researchers, using deep reinforcement learning, trained a drone to navigate through corridors using only simulation images and achieved decent success on flying through the hallways of Cory Hall. By using simulation images, the authors exposed the drone to many complex environments and were able to “reset” the environment when the drone crashed in simulation without any damage to the drone in real life.
</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/nXBWmzFrj5s" frameborder="0" allowfullscreen></iframe>

<h3>Deep Learning on Tensorflow and Keras</h3>

<img src=”https://blog.keras.io/img/keras-tensorflow-logo.jpg”>

<p>
Tensorflow is an open source software library for numerical computation developed by researchers at Google. Unfortunately, Tensorflow can be quite challenging to pick up for beginners. In this work, we use Keras, developed by another Googler Francois Chollet, which is a wrapper library for Tensorflow and Theano, and is much easier to use and understand. By using Keras, we make the trade-off for accessibility over nice Tensorflow features such as Tensorboard, which allows you to visualize the performance of models while training.
</p>

	  ]]></description>
	</item>

	<item>
	  <title>Building an Enigma Machine</title>
	  <link>//enigma</link>
	  <author>Brian Su</author>
	  <pubDate>2016-06-06T00:00:00+00:00</pubDate>
	  <guid>//enigma</guid>
	  <description><![CDATA[
	     <p><i>This post is still a work in progress!</i></p>

<p>During World War II, the Enigma machine was widely used by the German forces to communicate
    sensitive information amongst themselves. Believed by the Germans to be unbreakable,
    the Enigma had a series of rotors and a single reflector at the end. The first
    rotor would map the initial input into an output letter, which is then passed
    to the subsequent rotors in similar fashion. Upon reaching the reflector, which
    also maps its input to an output, the translation happens in reverse, where the
    output of the reflector is passed through the last rotor all the way to the first
    rotor, where it is then given back as an encoded letter. To decode, the encoded
    message would be passed through the Enigma machine with the same initial state,
    and the original message would show up.</p>

<img src="https://raw.githubusercontent.com/bsuper/bsuper-jasper/master/assets/images/enigma/enigma1.png">

<p>Okay, I simplified it a little bit. Before every letter is processed by the rotors,
    the first rotor rotates, and possibly the subsequent rotors too. Every rotor
    has a notch that it rotates the next rotor on. For example, Rotor II has notch
    E. This is predefined - otherwise, none of the Enigma operators would be able
    to transcribe messages from their counterparts. Thus, when Rotor II is on notch
    E, the next input that rotates Rotor II causes the subsequent rotor to rotate,
    increasing its notch. On the other hand, reflectors don't rotate.</p>

<p>There's also the ring offset. Suppose we are abou to pass in A to a rotor and the
    rotor is at notch C. In this case, the rotor treats A as C and outputs the mapping
    for C, instead of A.</p>

<p>In addition, there's also the plugboard (<i>Steckerbrett</i>) that is used before
    the input is passed into the first rotor. Basically, the operator has 13 wires
    that can change any two pairs of letters. For example, an operator can switch
    A for Z and Z for A.</p>

<p>Also, quick note about the reflector. It's mapping is reversable, so if A maps to
    D, then D maps to A.</p>

<p>Hello Cem!</p>

	  ]]></description>
	</item>


</channel>
</rss>
