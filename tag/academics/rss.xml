<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>blog.bsu.me/</title>
   
   <link>http://blog.bsu.me/</link>
   <description>A place where I'll share my latest travel stories, academic thoughts, and personal musings.</description>
   <language>en-us</language>
   <managingEditor> Brian Su</managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>Prioritized Experience Replay Kills on Doom</title>
	  <link>//294-per-doom</link>
	  <author>Brian Su</author>
	  <pubDate>2016-12-15T00:00:00+00:00</pubDate>
	  <guid>//294-per-doom</guid>
	  <description><![CDATA[
	     <h3> Why play video games? </h3>

<p>You may wonder why companies like Google and Facebook have research teams that try to outperform others (and themselves) on video games, such as Atari or Doom. I mean, why would some of the best minds of our day spend hours tackling how to be better at video games?</p>

<p>Turns out, the eventual goal is to use reinforcement learning to allow robots and other machines to interact with complex environments. In short, games are a simplified abstraction of complex environments machines have to learn and adapt to. Once we have successfully trained a model to play a game, applying the model to a real-world problem is a natural extension. For example, <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf ">Google’s DeepMind has trained reinforcement learning models to master Atari games</a>, <a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html ">beat a 9-dan Go professional</a>, and, more recently, <a href="https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/">reduced the power consumption of Google’s data centers by 15%</a>. Moreover, games provide two advantages when used for deep reinforcement learning.</p>

<p>First, since machines can quickly run through games, we are able to accumulate much more data from playing games compared to attempting to collect data in the real-world. In particular, this is beneficial when using deep neural networks as they require lots of data to train.</p>

<p>Second, games provide a visual representation of an environment. When humans play games, they learn how to play and make decisions based on what they see on the screen. Cameras are machines’ equivalent of human eyes, capturing a representation of the environment as an image, and, using machine learning, we can detect objects in the images. Thus, for humans and these machine learning models, the input is the raw pixels from images. For the task of deep reinforcement learning, most games provide a visual representation capturing most of the state of the game. Like how humans learn to play games, the goal is to use deep reinforcement learning and allow machines only use the visual representation to successfully complete the game. This is often referred to as end-to-end training.</p>

<p><a href="https://fsadeghi.github.io/CAD2RL/">(CAD)2RL</a> is one recent example where researchers, using deep reinforcement learning, trained a drone to navigate through corridors using only simulation images and achieved decent success on flying through the hallways of Cory Hall. By using simulation images, the authors exposed the drone to many complex environments and were able to “reset"the environment when the drone crashed in simulation without any damage to the drone in real life.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/nXBWmzFrj5s" frameborder="0" allowfullscreen></iframe>

<h3> Deep Reinforcement Learning, Deep Q-Learning,  and Double DQN </h3>

<p> Deep Q-Learning was first popularized by this paper written by Mnih et al. from DeepMind. In this work they built a deep q-learning agent and trained it to play 49 Atari games using only pixels as inputs. This work quickly captured attention because it was one of the earliest demonstrations of an end-to-end trained agent using a deep neural network and pixels as inputs to take actions in a wide range of environments.</p>

<p> Let’s quickly go over what we mean by deep q-learning.</p>

<p> A function Q(s,a) over a policy returns an estimate of a total reward we would obtain by taking that action a from state s and then following some optimal policy at the landing state s' let’s call it Q*(s,a). When we have finite states Q-learning converges and finds the optimal policy at every state. However most of the problems we are trying to solve have infinite states hence it is impossible to store these in a table to lookup and converge to an optimal policy. Instead the idea that was discussed in the DeepMind paper [6] was to use a deep neural network to approximate the Q function.</p>

<img src= "/assets/images/per/model_architecture.jpg"/>

<p> The idea here now is that given an experience $$(s,a,t,r,s’)$$ we need to approximate $$Q(s,a)$$ using our neural network. We will forward pass it through our network to get predicted Q-values for the state-action pair namely Q(s,a).  Next we need to compare this to our target value hence we will forward pass the landing state s' and compute the target Q value as:</p>

<p>$$Q^*(s,a) = r + \gamma max_{a'}Q(s',a')$$</p>

<p>To train our network we will use squared loss and do back propagation.</p>

<p>$$L = \frac{1}{2}[r + \gamma max_{a'}Q(s',a') - Q(s,a)]^2$$</p>

<p>
However one problem that authors of the DeepMind paper realised was that using a neural network to compute the Q function is unstable. To this end they used several stabilization methods with the goal of more stable policy learning. Here we are going to present a few of them as they will be useful for understanding what is to come.</p>

<h4> Learning from Experience </h4>

<p>One problem you might have guessed is that the experiences $$(s,a,t,r,s')$$ we use to train our network tend to be sequential in video games hence most of the times we will see the same or very similar states and states will be highly correlated. This means that we will update the q-values of experiences that are abundant but unimportant more often than important experiences. Another big problem is that as soon as we encounter an experience and use it to update our weights in the network we discard it. This is bad because less important experiences might come to us more often than more important experiences which will cause our network to  overfit.</p>

<p>To solve this, this paper proposes that we simply remember experiences by storing them in a replay memory and randomly selecting experiences from the replay memory to train our network instead of using the most recent one. This is very similar to what is done usually in supervised machine learning tasks.</p>

<h4> Two Separate Networks </h4>

<p> Currently in our loss calculations after we are using our deep q-network two times. First time in when we are computing the predicted q-value $$Q(s,a)$$ and second time to get the target q-value in $$r+maxa'Q(s',a')$$. The problem here is that since target q-value depends on the our current network state the target will shift each time we update our network. As it is shown in the original DeepMind paper this leads to oscillations in the q-values. To solve this they propose implementing a target q-network that is frozen in time so when we are using experiences to update our q-network the target network is used to get the target q-value and q-network is used to get the predicted q-value. This provides stability over time. In reality, target network is also going to be updated by copying the weights from our q-network but much less frequently.</p>


<h3> Double Deep Q Learning </h3>

<p>
 It took a lot of work for the DeepMind to be able to train a good network despite stability issues we just discussed. Perhaps a bigger problem they didn’t address in the original paper was the overestimation of q-values. This problem arises when we use the max function in target q-value computation.
</p>

<p>$$r + \gamma max_{a'}Q(s',a')$$</p>
<p>
When the prediction is noisy, this can happen in the beginning, and imagine that for state s'all the available actions have the same or similar q-values

<p>
$$Q(s',a'_i) \sim Q(s',a'_j)$$
$$\forall i,j$$
$$i\neq j $$
</p>




<p>This makes it susceptible to choosing wrong actions and propagating these q-values to compute other q-values will create a bias which will lead to overestimation of some q-values. In the paper by Hasselt et al. the proposed solution is to use two separate functions learned independently. Use the first function to maximize and second one to approximate the value. We already have two different networks that are different: the q-network and target q-network. Our new target q-value will be:</p>

	<p> $$Q^*(s,a) = r + \gamma Q^T(s', argmax_{a}Q(s',a))$$ </p>

<p>QT is computed from the target q-network and Q is computed from the q-network. This makes the learning process much more stable as it is shown in the paper that separating the target q-value computation by using two different q-value functions makes it unbiased.</p>


<h3> VIZDoom </h3>

<p>Doom is by far one of the most prominent games in history. It not only popularized FPS (First Person Shooter) games but also pioneered implementation of 3D graphics. Although when it was first released in 1993, it received much appreciation from the gaming world, nowadays people are interested in it mostly because it has a well defined game state to develop AI that can beat human players.</p>

<p>Vizdoom is an reinforcement learning based research platform that lets you implement intelligent agents that can play doom given only screen images from the game play. </p>

<img src="/assets/images/per/gameplay_snapshots.jpg"/>


<p>In this project, we used Vizdoom for a very limited number of tasks since we were only interested in grabbing the raw images of the game play to train and test. In reality, Vizdoom offers a variety of options such as creating your own game scenarios, single and multiplayer gameplay along with many other game environments. Vizdoom also offers API’s for C++ , Python and Java.</p>

<img src="/assets/images/per/scenarios.jpg"/>

<h3>Deep Learning on Tensorflow and Keras</h3>

<img src="https://blog.keras.io/img/keras-tensorflow-logo.jpg" />

<p>
Tensorflow is an open source software library for numerical computation developed by researchers at Google. Unfortunately, Tensorflow can be quite challenging to pick up for beginners. In this work, we use Keras, developed by another Googler Francois Chollet, which is a wrapper library for Tensorflow and Theano, and is much easier to use and understand. By using Keras, we make the trade-off for accessibility over nice Tensorflow features such as Tensorboard, which allows you to visualize the performance of models while training.
</p>

<h3> Prioritized Experience Replay on ViZDoom </h3>

<p>Everything we talked above was to prepare the reader for this part. Prioritized Experience Replay (PER) is a very novel idea that tries to solve a simple but fundamental problem with how we train our q-network. Previously we discussed the importance of storing experiences in a replay memory and randomly picking them to update our parameters. The original DeepMind paper mentioned that some transitions are more important than others and the fact that we are discarding them after using once can mean we might never experience them. Perhaps if we learned from the transitions or experiences that are more important than we might be able to learn faster.
</p>
<p>
We would like to learn from the transitions that doesn’t fit well with our current network prediction but in reality since we are randomly picking transitions from the replay memory we might not learn from important ones.
</p>

<p>For example in the famous breakout game some states are more important to visit while others are not as important. And important ones are much less frequently seen than the less important ones. Take a look at the following t-SNE visualization of the breakout game states.</p>

<img src="/assets/images/per/tsne.jpg"/>

<p>To quantify transitions that don’t fit well with our current network prediction we will look at the temporal difference (TD) error that simply computes the difference between the target network prediction and q-network prediction to assign a priority over the transitions. The bigger the error the higher the priority.</p>

<p>$$TD_error = |Q(s,a) - Q^*(s,a)|$$ where $$Q*(s,a) = r + \gamma Q^T(s', argmax_aQ(s',a))$$</p>

<p>Notice here that we are using DDQN update to make our learning more stable as it was also used in the paper by Schaul et al.</p>

<p>Previously we uniformly sampled experiences from the replay memory but this time in order to account for the priority of each experience we will translate this priority into a probability and we will sample from the replay memory using this probability distribution. An experience $$i$$ has the following probability of being picked from the replay memory:</p>

<p>
$$ P_i = \frac{p_i}{\sum_{k}p_k}$$
</p>

<p>
where $$p_i$$ represents the proportional priority of experience i:
</p>

<p>
$$p_i = [TD_error(i) - \epsilon]^\alpha$$
</p>


<p>At each step we will be picking an experience from the replay memory using this sampling probability. Experiences that have a higher TD error will be picked with higher probability.</p>

<h4> Implementation and Models </h4>

<p>Notice that implementation of PER is not very different than the implementation used in the DDQN paper by Hasselt et al. The only thing that is different is that the replay memory will not be a list storing experiences. We will use a priority queue to store the experiences to account for their priorities. This code sample puts a transition or an experience into our replay memory implemented as a priority queue.

<pre><code>def remember(self, transition, game_over):
  transition_powered_priority = 1e-7 ** self.alpha
  if self.memory:
    transition_powered_priority = np.max(self.memory, 0)[2]
  self.sum_powered_priorities += transition_powered_priority
  # insert the transition with its priority into replay memory
  self.memory.append([transition, game_over, transition_powered_priority])
  if len(self.memory) > self.max_memory:
    self.sum_powered_priorities -= self.memory[0][2]
    def self.memory[0]
</code></pre>

<h4> Algorithm </h4>
Here you can see the algorithm that was discussed in the PER paper.

<img src="/assets/images/per/dqn_algo.jpg"/>

<h4>Preprocessing Steps</h4>
<ol>
<li>The raw frames are preprocessed by first converting their RGB representation to gray-scale and then down-sampling to 110×84 images.</li>
<li>Crop an 84 × 84 region of the image that roughly captures the playing area.</li>
</ol>

<h4> Training and Metrics </h4>

<p>We trained our models on two different doom games: Basic and Defend the Line. Each of these game modes has a different way of quantifying rewards and penalties.</p>

<h5>
Basic Game
</h5>

<img src="/assets/images/per/basic.jpg"/>

<p>
  <ul>
    <li>
      Goal: Kill the monster in 3 seconds with 1 shot
    </li>
    <li>
      Metrics:
      <ul>
        <li>
          +101 reward for killing the monster
        </li>
        <li>
          -5 reward for missing a shot
        </li>
        <li>
          -1 reward for every 0.028 seconds that passes
        </li>
      </ul>
    </li>
  </ul>
</p>


<h5>
Defend the Line Game
</h5>

<img src="/assets/images/per/defend_the_line.jpg"/>

<p>
  <ul>
    <li>
      Goal: Kill 16 monsters
    </li>
    <li>
      Metrics:
      <ul>
        <li>
          +1 for killing a monster
        </li>
        <li>
          -1 for getting killed
        </li>
      </ul>
    </li>
  </ul>
</p>

<p>One important difference to mention here is that defend the line game trains much faster than basic. The reason for that is, there are more monsters and the agent gains rewards more frequently than in basic. We will see how this difference affects the performance of our models in the next section.</p>

<p>As mentioned in the implementation part, we used two different models to train each of these games. The first one is our baseline DQN model implemented using [8]. Second one is the double q learning model [9]. </p>

<p>
Training on Defend the Line:
</p>
<p>
For both baseline and double q learning models, we trained around 2000 episodes, max 300 steps per episode (a game ends after 300 steps if the agent is still not dead). We used an epsilon greedy exploration policy with a learning rate of 2.5e-4 and discounted rewards with a rate of 0.99.
</p>
<p>
Especially for the double dqn model with prioritized experience replay, we stored 1000 transition step vectors in memory. Training took around 8 hours on an Nvidia GTX 1070 gpu.
</p>
<p>
Training on Basic:
</p>
<p>For both models, we trained around 10000 episodes, with a max number of 40 steps per episode. We used the same discount rate and learning rate and for our double dqn, same memory size as defend the line. For time constraints, we only trained it around 1 day and unfortunately results didn’t converge too much like defend the line. </p>

<h4> Performance and Results </h4>

<p>Below, are the trained models performing on the defend the line game environment.</p>

<p>And here is the baseline performance</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/qur6oCQo4fk" frameborder="0" allowfullscreen></iframe>

<p>Here is the prioritized replay performance.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/OY3JmoifOhU" frameborder="0" allowfullscreen></iframe>

<p>
For the entirety of this section, I will focus on the game defend the line since we had enough time for a good model to train.
</p>

<p>Performance differences between the baseline DQN model and DDQN with Prioritized Experience Replay show that Prioritized Experience Replay enables faster learning and higher performance.</p>


          	<img src"/assets/images/per/reward_comparison.jpg"/>
<p>
The plots above show that given the same amount of training time, double dqn model converges to much faster than the baseline model to higher rewards. Baseline model gets stuck around a reward of 10 while the game expects us to achieve a reward of 15 (Kill all monsters and die).
</p>
<p>
Our results from training a prioritized experience replay on DDQN for defend the line game show that we obtain higher rewards, faster training times and less stochastic average return performance compared to the baseline dqn model. Given the complicated nature of Doom with a variety of maps, items to be picked up (health kits) and different monsters to kill, we expect prioritized experience replay to have a noticeable impact on complex game modes such as “deathmatch.”
</p>

<h3> Final Thoughts and Future Work </h3>
<p>
Prioritized Experience Replay is a method that rewards an agent for exploring its environment. Intuitively, by encouraging more exploration, the reinforcement learning agent will stumble onto novel kinds of environments that allow for greater rewards. Using Prioritized Experience Replay, the agent trains more frequently on experiences that yielded “unexpected"rewards. Our results from training a DDQN model with Prioritized Experience Replay confirm that this direction is promising.
</p>
<p>
For future work, we would like to evaluate Prioritized Experience Replay (PER) on more complicated game modes of Doom such as “deathmatch."In addition, we plan to compare that with recent work on “asynchronous advantage actor critic (A3C) on Doom”. Based on its paper, A3C should allow us to reduce our reliance on GPUs. Since DDQN training with PER is a GPU memory intensive process, we think this would be an interesting  comparison.
</p>

<h3> Team Contributions </h3>

<h5>Can Koc (⅓ of the work)</h5>

<p>Can mainly worked on training the baseline and DDQN with PER models on the two doom games. He also worked on getting the reward plots and performance metrics. He also worked with Cem to build a gpu computer from scratch.</p>

<h5>Cem Koc (⅓ of the work) </h5>

<p>Cem mainly worked on with Brian Su on implementing the prioritized experience replay on ddq and baseline dqn code and debugging it. He also worked on setting up the computer for development.</p>

<h5>Brian Su (⅓ of the work)</h5>

<p>Brian Su worked on implementing the DDQN and prioritized experience replay with Cem Koc. He also worked on formatting the report to turn it into a blog post.</p>

	  ]]></description>
	</item>


</channel>
</rss>
